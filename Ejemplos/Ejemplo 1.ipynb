{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All Required Libraries\n",
    "import math\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.color_palette(\"bright\")\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch.nn  import functional as F \n",
    "from torch.autograd import Variable\n",
    "\n",
    "use_cuda = torch.cuda.is_available() #Check if it is possible to use GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Toy Example of ODE Solver (Euler IV Solver)\n",
    "def ode_solve(z0, t0, t1, f):\n",
    "    h_max = 0.05\n",
    "    n_steps = math.ceil((abs(t1 - t0)/h_max).max().item())\n",
    "\n",
    "    h = (t1 - t0)/n_steps\n",
    "    t = t0\n",
    "    z = z0\n",
    "\n",
    "    for i_step in range(n_steps):\n",
    "        z = z + h * f(z, t)\n",
    "        t = t + h\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculates Forward Pass and Gradients of the Loss function wrt z,t,parameters.\n",
    "#Compute f and a df/dz, a df/dp, a df/dt (i.e., da(t)/dt)\n",
    "class ODEF(nn.Module):\n",
    "    def forward_with_grad(self, z, t, grad_outputs):\n",
    "        batch_size = z.shape[0]\n",
    "\n",
    "        out = self.forward(z, t) #Calculates the forward pass of the model.\n",
    "\n",
    "        a = grad_outputs #Gradient of the model's output (out) wrt z, t, and all the model parameters.\n",
    "        adfdz, adfdt, *adfdp = torch.autograd.grad(\n",
    "            (out,), (z, t) + tuple(self.parameters()), grad_outputs=(a),\n",
    "            allow_unused=True, retain_graph=True\n",
    "        )\n",
    "        # grad method automatically sums gradients for batch items, we have to expand them back \n",
    "        if adfdp is not None:\n",
    "            adfdp = torch.cat([p_grad.flatten() for p_grad in adfdp]).unsqueeze(0) #Creates a list of flattened parameter gradients.\n",
    "            adfdp = adfdp.expand(batch_size, -1) / batch_size #Create the gradient tensor for each batchsize, second dim is unchanged (i.e., 1 for each parameter).\n",
    "        if adfdt is not None:\n",
    "            adfdt = adfdt.expand(batch_size, 1) / batch_size #Same as for the parameters but the second dimension is 1 as there is only 1 \"parameter\".\n",
    "        return out, adfdz, adfdt, adfdp\n",
    "\n",
    "    def flatten_parameters(self):\n",
    "        p_shapes = []\n",
    "        flat_parameters = []\n",
    "        for p in self.parameters():\n",
    "            p_shapes.append(p.size())\n",
    "            flat_parameters.append(p.flatten())\n",
    "        return torch.cat(flat_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculates Forward and Backward Pass and Gradients with Adjoint Method.\n",
    "class ODEAdjoint(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, z0, t, flat_parameters, func):\n",
    "        assert isinstance(func, ODEF) #Make sure func is an instance of ODEF.\n",
    "        bs, *z_shape = z0.size() #bs (batchsize), z_shape (shape of initial state z0) - The asterisk * is used to collect the remaining elements of the tuple (all except the first). \n",
    "        time_len = t.size(0)\n",
    "\n",
    "        with torch.no_grad(): #Operations not tracked for gradient calculation.\n",
    "            z = torch.zeros(time_len, bs, *z_shape).to(z0) #Initialize z as zeros of t,bs,z in the same device (CPU/GPU) as z0.\n",
    "            z[0] = z0 #Define initial value.\n",
    "            for i_t in range(time_len - 1):\n",
    "                z0 = ode_solve(z0, t[i_t], t[i_t+1], func) #Solve ODE with shape f(z(0),t_i,t_f) in steps.\n",
    "                z[i_t+1] = z0\n",
    "\n",
    "        ctx.func = func #Store the function to use it later in the Backpropagation,\n",
    "        ctx.save_for_backward(t, z.clone(), flat_parameters) #Same for the tensors for time, trajectory (z) and parameters.\n",
    "        return z\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dLdz): #a(t) = dLdz\n",
    "        #dLdz shape: time_len, batch_size, *z_shape\n",
    "        func = ctx.func #This is the ODE Function.\n",
    "        t, z, flat_parameters = ctx.saved_tensors #Our saved tensors from Forward Pass.\n",
    "        time_len, bs, *z_shape = z.size()\n",
    "        n_dim = np.prod(z_shape) #Number of dimensions of z.\n",
    "        n_params = flat_parameters.size(0) #Number of parameters.\n",
    "\n",
    "        # Dynamics of augmented system to be calculated backwards in time\n",
    "        def augmented_dynamics(aug_z_i, t_i):\n",
    "            \"\"\"\n",
    "            tensors here are temporal slices\n",
    "            t_i - is tensor with size: bs, 1\n",
    "            aug_z_i - is tensor with size: bs, n_dim*2 + n_params + 1\n",
    "            \"\"\"\n",
    "            z_i, a = aug_z_i[:, :n_dim], aug_z_i[:, n_dim:2*n_dim]  # ignore parameters and time\n",
    "\n",
    "            # Unflatten z and a\n",
    "            z_i = z_i.view(bs, *z_shape)\n",
    "            a = a.view(bs, *z_shape)\n",
    "            with torch.set_grad_enabled(True):\n",
    "                t_i = t_i.detach().requires_grad_(True)\n",
    "                z_i = z_i.detach().requires_grad_(True)\n",
    "                func_eval, adfdz, adfdt, adfdp = func.forward_with_grad(z_i, t_i, grad_outputs=a)  # bs, *z_shape\n",
    "                adfdz = adfdz.to(z_i) if adfdz is not None else torch.zeros(bs, *z_shape).to(z_i)\n",
    "                adfdp = adfdp.to(z_i) if adfdp is not None else torch.zeros(bs, n_params).to(z_i)\n",
    "                adfdt = adfdt.to(z_i) if adfdt is not None else torch.zeros(bs, 1).to(z_i)\n",
    "\n",
    "            # Flatten f and adfdz\n",
    "            func_eval = func_eval.view(bs, n_dim)\n",
    "            adfdz = adfdz.view(bs, n_dim) \n",
    "            return torch.cat((func_eval, -adfdz, -adfdp, -adfdt), dim=1)\n",
    "\n",
    "        dLdz = dLdz.view(time_len, bs, n_dim)  # flatten dLdz for convenience\n",
    "        with torch.no_grad():\n",
    "            ## Create placeholders for output gradients\n",
    "            # Prev computed backwards adjoints to be adjusted by direct gradients\n",
    "            adj_z = torch.zeros(bs, n_dim).to(dLdz)\n",
    "            adj_p = torch.zeros(bs, n_params).to(dLdz)\n",
    "            # In contrast to z and p we need to return gradients for all times\n",
    "            adj_t = torch.zeros(time_len, bs, 1).to(dLdz)\n",
    "\n",
    "            for i_t in range(time_len-1, 0, -1):\n",
    "                z_i = z[i_t]\n",
    "                t_i = t[i_t]\n",
    "                f_i = func(z_i, t_i).view(bs, n_dim)\n",
    "\n",
    "                # Compute direct gradients\n",
    "                dLdz_i = dLdz[i_t]\n",
    "                dLdt_i = torch.bmm(torch.transpose(dLdz_i.unsqueeze(-1), 1, 2), f_i.unsqueeze(-1))[:, 0]\n",
    "\n",
    "                # Adjusting adjoints with direct gradients\n",
    "                adj_z += dLdz_i\n",
    "                adj_t[i_t] = adj_t[i_t] - dLdt_i\n",
    "\n",
    "                # Pack augmented variable\n",
    "                aug_z = torch.cat((z_i.view(bs, n_dim), adj_z, torch.zeros(bs, n_params).to(z), adj_t[i_t]), dim=-1)\n",
    "\n",
    "                # Solve augmented system backwards\n",
    "                aug_ans = ode_solve(aug_z, t_i, t[i_t-1], augmented_dynamics)\n",
    "\n",
    "                # Unpack solved backwards augmented system\n",
    "                adj_z[:] = aug_ans[:, n_dim:2*n_dim]\n",
    "                adj_p[:] += aug_ans[:, 2*n_dim:2*n_dim + n_params]\n",
    "                adj_t[i_t-1] = aug_ans[:, 2*n_dim + n_params:]\n",
    "\n",
    "                del aug_z, aug_ans\n",
    "\n",
    "            ## Adjust 0 time adjoint with direct gradients\n",
    "            # Compute direct gradients \n",
    "            dLdz_0 = dLdz[0]\n",
    "            dLdt_0 = torch.bmm(torch.transpose(dLdz_0.unsqueeze(-1), 1, 2), f_i.unsqueeze(-1))[:, 0]\n",
    "\n",
    "            # Adjust adjoints\n",
    "            adj_z += dLdz_0\n",
    "            adj_t[0] = adj_t[0] - dLdt_0\n",
    "        return adj_z.view(bs, *z_shape), adj_t, adj_p, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralODE(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super(NeuralODE, self).__init__()\n",
    "        assert isinstance(func, ODEF) #Checks if the argument func is part of the ODEF.\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, z0, t=Tensor([0., 1.]), return_whole_sequence=False):\n",
    "        t = t.to(z0) #It ensures that the time tensor t is on the same device as the input z0.\n",
    "        z = ODEAdjoint.apply(z0, t, self.func.flatten_parameters(), self.func) #Numerically integrating the ODE over the specified time range using an adjoint method (Backpropagation).\n",
    "        if return_whole_sequence:\n",
    "            return z\n",
    "        else:\n",
    "            return z[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearODEF(ODEF):\n",
    "    def __init__(self, W):\n",
    "        super(LinearODEF, self).__init__()\n",
    "        self.lin = nn.Linear(2, 2, bias=False)\n",
    "        self.lin.weight = nn.Parameter(W)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        return self.lin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpiralFunctionExample(LinearODEF):\n",
    "    def __init__(self):\n",
    "        super(SpiralFunctionExample, self).__init__(Tensor([[-0.1, -1.], [1., -0.1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomLinearODEF(LinearODEF):\n",
    "    def __init__(self):\n",
    "        super(RandomLinearODEF, self).__init__(torch.randn(2, 2)/2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestODEF(ODEF):\n",
    "    def __init__(self, A, B, x0):\n",
    "        super(TestODEF, self).__init__()\n",
    "        self.A = nn.Linear(2, 2, bias=False)\n",
    "        self.A.weight = nn.Parameter(A)\n",
    "        self.B = nn.Linear(2, 2, bias=False)\n",
    "        self.B.weight = nn.Parameter(B)\n",
    "        self.x0 = nn.Parameter(x0)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        xTx0 = torch.sum(x*self.x0, dim=1) #Dot product between x and x0, sum along dim 1.\n",
    "        dxdt = torch.sigmoid(xTx0) * self.A(x - self.x0) + torch.sigmoid(-xTx0) * self.B(x + self.x0) #ODE Dynamics\n",
    "        return dxdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNODEF(ODEF):\n",
    "    def __init__(self, in_dim, hid_dim, time_invariant=False):\n",
    "        super(NNODEF, self).__init__()\n",
    "        self.time_invariant = time_invariant\n",
    "\n",
    "        if time_invariant:\n",
    "            self.lin1 = nn.Linear(in_dim, hid_dim) #Doesn't include time.\n",
    "        else:\n",
    "            self.lin1 = nn.Linear(in_dim+1, hid_dim) #Includes time.\n",
    "        self.lin2 = nn.Linear(hid_dim, hid_dim)\n",
    "        self.lin3 = nn.Linear(hid_dim, in_dim)\n",
    "        self.elu = nn.ELU(inplace=True) #Exponential Linear Unit activation function after each linear layer.\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        if not self.time_invariant:\n",
    "            x = torch.cat((x, t), dim=-1) #If not time-invariant, concatenate time to state x.\n",
    "\n",
    "        h = self.elu(self.lin1(x)) #Input into first layer.\n",
    "        h = self.elu(self.lin2(h)) #Second layer.\n",
    "        out = self.lin3(h) #Last layer.\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detaches a PyTorch tensor from its computation graph, moves it to the CPU if it's on the GPU, and then converts it to a NumPy array.\n",
    "def to_np(x):\n",
    "    return x.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trajectories(obs=None, times=None, trajs=None, save=None, figsize=(16, 8)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    if obs is not None:\n",
    "        if times is None:\n",
    "            times = [None] * len(obs)\n",
    "        for o, t in zip(obs, times):\n",
    "            o, t = to_np(o), to_np(t)\n",
    "            for b_i in range(o.shape[1]):\n",
    "                plt.scatter(o[:, b_i, 0], o[:, b_i, 1], c=t[:, b_i, 0], cmap=cm.plasma)\n",
    "\n",
    "    if trajs is not None: \n",
    "        for z in trajs:\n",
    "            z = to_np(z)\n",
    "            plt.plot(z[:, 0, 0], z[:, 0, 1], lw=1.5)\n",
    "        if save is not None:\n",
    "            plt.savefig(save)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_experiment(ode_true, ode_trained, n_steps, name, plot_freq=10):\n",
    "    # Create data\n",
    "    z0 = Variable(torch.Tensor([[0.6, 0.3]]))\n",
    "\n",
    "    t_max = 6.29*5\n",
    "    n_points = 200\n",
    "\n",
    "    index_np = np.arange(0, n_points, 1, dtype=int)\n",
    "    index_np = np.hstack([index_np[:, None]])\n",
    "    times_np = np.linspace(0, t_max, num=n_points)\n",
    "    times_np = np.hstack([times_np[:, None]])\n",
    "\n",
    "    times = torch.from_numpy(times_np[:, :, None]).to(z0)\n",
    "    obs = ode_true(z0, times, return_whole_sequence=True).detach()\n",
    "    obs = obs + torch.randn_like(obs) * 0.01\n",
    "\n",
    "    # Get trajectory of random timespan \n",
    "    min_delta_time = 1.0\n",
    "    max_delta_time = 5.0\n",
    "    max_points_num = 32\n",
    "    def create_batch():\n",
    "        t0 = np.random.uniform(0, t_max - max_delta_time)\n",
    "        t1 = t0 + np.random.uniform(min_delta_time, max_delta_time)\n",
    "\n",
    "        idx = sorted(np.random.permutation(index_np[(times_np > t0) & (times_np < t1)])[:max_points_num])\n",
    "\n",
    "        obs_ = obs[idx]\n",
    "        ts_ = times[idx]\n",
    "        return obs_, ts_\n",
    "\n",
    "    # Train Neural ODE\n",
    "    optimizer = torch.optim.Adam(ode_trained.parameters(), lr=0.01)\n",
    "    for i in range(n_steps):\n",
    "        obs_, ts_ = create_batch()\n",
    "\n",
    "        z_ = ode_trained(obs_[0], ts_, return_whole_sequence=True)\n",
    "        loss = F.mse_loss(z_, obs_.detach())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % plot_freq == 0:\n",
    "            z_p = ode_trained(z0, times, return_whole_sequence=True)\n",
    "\n",
    "            plot_trajectories(obs=[obs], times=[times], trajs=[z_p], save=None)\n",
    "            clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ode_true = NeuralODE(SpiralFunctionExample())\n",
    "ode_trained = NeuralODE(RandomLinearODEF())\n",
    "conduct_experiment(ode_true, ode_trained, 1000, \"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = TestODEF(Tensor([[-0.1, -0.5], [0.5, -0.1]]), Tensor([[0.2, 1.], [-1, 0.2]]), Tensor([[-1., 0.]]))\n",
    "ode_true = NeuralODE(func)\n",
    "func = NNODEF(2, 16, time_invariant=True)\n",
    "ode_trained = NeuralODE(func)\n",
    "conduct_experiment(ode_true, ode_trained, 3000, \"comp\", plot_freq=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealGridODE(ODEF):\n",
    "    def __init__(self):\n",
    "        super(RealGridODE, self).__init__()\n",
    "        self.M = Tensor([[1]])\n",
    "        self.D = Tensor([[1]])\n",
    "        self.B = Tensor([[1]])\n",
    "        self.G = Tensor([[1]])\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        x_prime = x[:,1]\n",
    "        x_prime_prime = (-self.D * x_prime + self.B * torch.sin(x[:, 0]) - self.G * torch.cos(x[:, 0])) / self.M\n",
    "        return torch.stack((x_prime, x_prime_prime), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EstimateGridODE(ODEF):\n",
    "    def __init__(self,M,D,B,G):\n",
    "        super(EstimateGridODE, self).__init__()\n",
    "        self.M = nn.Parameter(M)\n",
    "        self.D = nn.Parameter(D)\n",
    "        self.B = nn.Parameter(B)\n",
    "        self.G = nn.Parameter(G)\n",
    "        \n",
    "    def forward(self, y):\n",
    "        y_prime = y[:,1]\n",
    "        y_prime_prime = (-self.D * y_prime + self.B * torch.sin(y[:, 0]) - self.G * torch.cos(y[:, 0])) / self.M\n",
    "        return torch.stack((y_prime, y_prime_prime), dim=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "torch.cat(): expected a non-empty list of Tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/morispolanco/Documents/GitHub/torchdiffeq/Ejemplos/Ejemplo 1.ipynb Cell 19\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/morispolanco/Documents/GitHub/torchdiffeq/Ejemplos/Ejemplo%201.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m ode_true \u001b[39m=\u001b[39m NeuralODE(RealGridODE(\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/morispolanco/Documents/GitHub/torchdiffeq/Ejemplos/Ejemplo%201.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m ode_trained \u001b[39m=\u001b[39m NeuralODE(EstimateGridODE(torch\u001b[39m.\u001b[39mrand(\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m), torch\u001b[39m.\u001b[39mrand(\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m), torch\u001b[39m.\u001b[39mrand(\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m),torch\u001b[39m.\u001b[39mrand(\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m)))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/morispolanco/Documents/GitHub/torchdiffeq/Ejemplos/Ejemplo%201.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m conduct_experiment(ode_true, ode_trained, \u001b[39m100\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mExperiment\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32m/Users/morispolanco/Documents/GitHub/torchdiffeq/Ejemplos/Ejemplo 1.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/morispolanco/Documents/GitHub/torchdiffeq/Ejemplos/Ejemplo%201.ipynb#X26sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m times_np \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mhstack([times_np[:, \u001b[39mNone\u001b[39;00m]])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/morispolanco/Documents/GitHub/torchdiffeq/Ejemplos/Ejemplo%201.ipynb#X26sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m times \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(times_np[:, :, \u001b[39mNone\u001b[39;00m])\u001b[39m.\u001b[39mto(z0)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/morispolanco/Documents/GitHub/torchdiffeq/Ejemplos/Ejemplo%201.ipynb#X26sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m obs \u001b[39m=\u001b[39m ode_true(z0, times, return_whole_sequence\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39mdetach()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/morispolanco/Documents/GitHub/torchdiffeq/Ejemplos/Ejemplo%201.ipynb#X26sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m obs \u001b[39m=\u001b[39m obs \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39mrandn_like(obs) \u001b[39m*\u001b[39m \u001b[39m0.01\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/morispolanco/Documents/GitHub/torchdiffeq/Ejemplos/Ejemplo%201.ipynb#X26sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Get trajectory of random timespan \u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/morispolanco/Documents/GitHub/torchdiffeq/Ejemplos/Ejemplo 1.ipynb Cell 19\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/morispolanco/Documents/GitHub/torchdiffeq/Ejemplos/Ejemplo%201.ipynb#X26sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, z0, t\u001b[39m=\u001b[39mTensor([\u001b[39m0.\u001b[39m, \u001b[39m1.\u001b[39m]), return_whole_sequence\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/morispolanco/Documents/GitHub/torchdiffeq/Ejemplos/Ejemplo%201.ipynb#X26sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     t \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mto(z0) \u001b[39m#It ensures that the time tensor t is on the same device as the input z0.\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/morispolanco/Documents/GitHub/torchdiffeq/Ejemplos/Ejemplo%201.ipynb#X26sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     z \u001b[39m=\u001b[39m ODEAdjoint\u001b[39m.\u001b[39mapply(z0, t, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc\u001b[39m.\u001b[39;49mflatten_parameters(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc) \u001b[39m#Numerically integrating the ODE over the specified time range using an adjoint method (Backpropagation).\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/morispolanco/Documents/GitHub/torchdiffeq/Ejemplos/Ejemplo%201.ipynb#X26sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mif\u001b[39;00m return_whole_sequence:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/morispolanco/Documents/GitHub/torchdiffeq/Ejemplos/Ejemplo%201.ipynb#X26sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m z\n",
      "\u001b[1;32m/Users/morispolanco/Documents/GitHub/torchdiffeq/Ejemplos/Ejemplo 1.ipynb Cell 19\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/morispolanco/Documents/GitHub/torchdiffeq/Ejemplos/Ejemplo%201.ipynb#X26sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     p_shapes\u001b[39m.\u001b[39mappend(p\u001b[39m.\u001b[39msize())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/morispolanco/Documents/GitHub/torchdiffeq/Ejemplos/Ejemplo%201.ipynb#X26sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     flat_parameters\u001b[39m.\u001b[39mappend(p\u001b[39m.\u001b[39mflatten())\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/morispolanco/Documents/GitHub/torchdiffeq/Ejemplos/Ejemplo%201.ipynb#X26sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mcat(flat_parameters)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: torch.cat(): expected a non-empty list of Tensors"
     ]
    }
   ],
   "source": [
    "ode_true = NeuralODE(RealGridODE())\n",
    "ode_trained = NeuralODE(EstimateGridODE(torch.rand(1,1), torch.rand(1,1), torch.rand(1,1),torch.rand(1,1)))\n",
    "conduct_experiment(ode_true, ode_trained, 100, \"Experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomODEF(ODEF):\n",
    "    def __init__(self, M, D, B, G):\n",
    "        super(CustomODEF, self).__init__()\n",
    "        self.M = M\n",
    "        self.D = D\n",
    "        self.B = B\n",
    "        self.G = G\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # Extract x and its derivative x'\n",
    "        x_val, x_prime = x[:, 0], x[:, 1]\n",
    "\n",
    "        # Compute the second derivative of x using the given ODE\n",
    "        x_double_prime = (1.0 / self.M) * (-self.D * x_prime + self.B * torch.sin(x_val) - self.G * torch.cos(x_val))\n",
    "\n",
    "        # Stack x' and x'' to form a new state\n",
    "        dxdt = torch.stack([x_prime, x_double_prime], dim=1)\n",
    "\n",
    "        return dxdt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "torch.cat(): expected a non-empty list of Tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/morispolanco/Documents/GitHub/torchdiffeq/Ejemplos/Ejemplo 1.ipynb Cell 20\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/morispolanco/Documents/GitHub/torchdiffeq/Ejemplos/Ejemplo%201.ipynb#X32sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Step 4: Solve the ODE using Neural ODE\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/morispolanco/Documents/GitHub/torchdiffeq/Ejemplos/Ejemplo%201.ipynb#X32sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m ode_trained \u001b[39m=\u001b[39m NeuralODE(ode_func)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/morispolanco/Documents/GitHub/torchdiffeq/Ejemplos/Ejemplo%201.ipynb#X32sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m trajectory \u001b[39m=\u001b[39m ode_trained(z0, t\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mTensor(t_span), return_whole_sequence\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/morispolanco/Documents/GitHub/torchdiffeq/Ejemplos/Ejemplo%201.ipynb#X32sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Step 5: Plot the trajectory\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/morispolanco/Documents/GitHub/torchdiffeq/Ejemplos/Ejemplo%201.ipynb#X32sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m8\u001b[39m, \u001b[39m6\u001b[39m))\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/morispolanco/Documents/GitHub/torchdiffeq/Ejemplos/Ejemplo 1.ipynb Cell 20\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/morispolanco/Documents/GitHub/torchdiffeq/Ejemplos/Ejemplo%201.ipynb#X32sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, z0, t\u001b[39m=\u001b[39mTensor([\u001b[39m0.\u001b[39m, \u001b[39m1.\u001b[39m]), return_whole_sequence\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/morispolanco/Documents/GitHub/torchdiffeq/Ejemplos/Ejemplo%201.ipynb#X32sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     t \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mto(z0) \u001b[39m#It ensures that the time tensor t is on the same device as the input z0.\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/morispolanco/Documents/GitHub/torchdiffeq/Ejemplos/Ejemplo%201.ipynb#X32sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     z \u001b[39m=\u001b[39m ODEAdjoint\u001b[39m.\u001b[39mapply(z0, t, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc\u001b[39m.\u001b[39;49mflatten_parameters(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc) \u001b[39m#Numerically integrating the ODE over the specified time range using an adjoint method (Backpropagation).\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/morispolanco/Documents/GitHub/torchdiffeq/Ejemplos/Ejemplo%201.ipynb#X32sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mif\u001b[39;00m return_whole_sequence:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/morispolanco/Documents/GitHub/torchdiffeq/Ejemplos/Ejemplo%201.ipynb#X32sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m z\n",
      "\u001b[1;32m/Users/morispolanco/Documents/GitHub/torchdiffeq/Ejemplos/Ejemplo 1.ipynb Cell 20\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/morispolanco/Documents/GitHub/torchdiffeq/Ejemplos/Ejemplo%201.ipynb#X32sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     p_shapes\u001b[39m.\u001b[39mappend(p\u001b[39m.\u001b[39msize())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/morispolanco/Documents/GitHub/torchdiffeq/Ejemplos/Ejemplo%201.ipynb#X32sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     flat_parameters\u001b[39m.\u001b[39mappend(p\u001b[39m.\u001b[39mflatten())\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/morispolanco/Documents/GitHub/torchdiffeq/Ejemplos/Ejemplo%201.ipynb#X32sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mcat(flat_parameters)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: torch.cat(): expected a non-empty list of Tensors"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Create an instance of CustomODEF with your constants\n",
    "M = 1.0\n",
    "D = 0.1\n",
    "B = 1.0\n",
    "G = 0.5\n",
    "\n",
    "ode_func = CustomODEF(M, D, B, G)\n",
    "\n",
    "# Step 2: Define the time span\n",
    "t_span = np.linspace(0, 10, 1000)  # Adjust the time span as needed\n",
    "\n",
    "# Step 3: Set initial conditions\n",
    "x0 = 1.0  # Initial position\n",
    "v0 = 0.0  # Initial velocity\n",
    "z0 = Variable(torch.Tensor([[x0, v0]]))  # Initial state\n",
    "\n",
    "# Step 4: Solve the ODE using Neural ODE\n",
    "ode_trained = NeuralODE(ode_func)\n",
    "trajectory = ode_trained(z0, t=torch.Tensor(t_span), return_whole_sequence=True).detach().numpy()\n",
    "\n",
    "# Step 5: Plot the trajectory\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(t_span, trajectory[:, 0], label='Position (x)')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Position')\n",
    "plt.title('Solution of the Differential Equation')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
